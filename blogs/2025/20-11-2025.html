<!DOCTYPE html>
<html lang="">

<head>
    <meta name="generator" content="Hugo 0.148.2">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>Dawood Sarfraz</title>
    <link rel="icon" href='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><text y="0.9em" font-size="90" fill="green">üåê</text></svg>'>
    <meta name="description" content="Personal webpage of Dawood Sarfraz">
    <meta name="author" content='Dawood Sarfraz'>

    <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/4.6.2/css/bootstrap.min.css" integrity="sha512-rt/SrQ4UNIaGfDyEXZtNcyWvQeOq0QLygHluFQcSjaGB04IxWhal71tKuzP6K8eYXYB6vJV4pHkXcmFGGQ1/0w==" crossorigin="anonymous" referrerpolicy="no-referrer"
    />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin="anonymous" referrerpolicy="no-referrer"
    />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.3/css/academicons.min.css" integrity="sha512-vaoopdl+FJahyY2ddhsbDj8yDiRuyUYH/vIjF3z+cBg0sKc07NAQmUYli8volCGlW9OwlQyjVsr7Lh6qAManlw==" crossorigin="anonymous" referrerpolicy="no-referrer"
    />
    <link rel="stylesheet" href="../../../assests/sass/researcher.min.css"  >
    <link rel="alternate" type="application/rss+xml" href="https://dawoodsarfraz.github.io/" title="Dawood Sarfraz" />
</head>

<body>
    <div class="container mt-5">
        <nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0">
            <a class="navbar-brand mx-0 mr-sm-auto" href="https://dawoodsarfraz.github.io/" title="Dawood Sarfraz">
          
          Dawood Sarfraz
        </a>
            <div class="navbar-nav flex-row flex-wrap justify-content-center">



                <a class="nav-item nav-link" href="../../index.html" title="Me">
                        Me
                    </a>
                    
                <span class="nav-item navbar-text mx-1">/</span>
                <a class="nav-item nav-link" href="../../index.html#experience" title="Experience">
                        Exp
                    </a>

                <span class="nav-item navbar-text mx-1">/</span>
                <a class="nav-item nav-link" href="../../index.html#projects" title="Projects">
                        Proj
                    </a>

                <span class="nav-item navbar-text mx-1">/</span>
                <a class="nav-item nav-link" href="../../index.html#publications" title="Publications">
                        Pub
                    </a>

                <span class="nav-item navbar-text mx-1">/</span>
                <a class="nav-item nav-link" href="../../index.html#miscellaneous" title="Miscellaneous">
                        Misc
                    </a>
            </div>
        </nav>
    </div>
    <hr>
    <div id="content">
        <div class="container">
            <div class="container text-center" id="about-links">
                <a href="mailto:dawoodsarfraz.cs@gmail.com" class="fas fa-envelope fa-1x" title="E-mail"></a>
                <a href="https://www.linkedin.com/in/dawood-sarfraz-0466541b6/" class="fab fa-linkedin fa-1x" title="LinkedIn" target="_blank"></a>
                <a href="https://github.com/Dawoodsarfraz" class="fab fa-github fa-1x" title="GitHub"></a>
                <a href="https://scholar.google.com" class="ai ai-google-scholar fa-1x" title="Google Scholar"></a>
                <a href="https://www.researchgate.net" class="ai ai-researchgate fa-1x" title="ResearchGate"></a>
                <a href="https://medium.com/@dawoodsarfraz0346" class="fab fa-medium fa-1x" title="Medium" target="_blank"></a>
                <a href="../../assests/cv/Dawood_Sarfraz_ML_CV.pdf" class="fas fa-file-alt fa-1x" title="CV" target="_blank"></a>
            </div>
            <br>
            dawoodsarfraz[dot]cs[@]gmail[dot]com &nbsp; | &nbsp; dawoodsarfraz0346[@]gmail[dot]com <br>
    </div>

    <!-- Blog Header Section -->
    <div class="container my-4">
        <h1 class="mb-3" style="font-weight:700; font-size:2rem;">
            Just a Demo Blog
        </h1>
        <div class="text-muted" style="font-size:0.95rem;">
            <span><strong>Authors:</strong> Dawood Sarfraz</span><br>
            <span><strong>Published:</strong> November 17, 2025</span><br>
            <span><strong>Reading Time:</strong> 8 minutes</span>
        </div>

        <hr>

        <!-- Blog Series Navigation -->
        <div class="my-3">
            <p class="mb-1"><strong>This blog post series accompanies our paper.</strong></p>

            <ul style="list-style:none; padding-left:0; margin-top:0.5rem;">
                <li><a href="./blog-part1.html">Part I ‚Äî The Model</a></li>
                <li><a href="./blog-part2.html">Part II ‚Äî The Algorithm</a></li>
                <li><a href="./blog-part3.html">Part III ‚Äî The Neural Architecture</a></li>
            </ul>
        </div>

        <hr>
    </div>

    <!-- Content Section (Collapsible Table of Contents) -->
    <div class="container mt-4">
        <button class="btn btn-outline-dark w-100 text-left" type="button" 
                data-toggle="collapse" data-target="#contentBox"
                aria-expanded="false" aria-controls="contentBox">
            <i class="fas fa-chevron-down mr-2"></i> Content
        </button>

        <div class="collapse mt-3" id="contentBox">
            <div class="card card-body">
                <ul style="padding-left:18px;">
                    <li><a href="#intro">Introduction</a></li>
                    <li><a href="#implement">Implementing Linear Attention</a></li>
                    <li><a href="#delta">Why Delta Rule Helps</a></li>
                    <li><a href="#code1">Python Math Example</a></li>
                    <li><a href="#refs">References</a></li>
                </ul>
            </div>
        </div>
    </div>


    <!-- BLOG CONTENT START -->
    <div class="container mt-4">

        <!-- Intro Section -->
        <h2 id="intro" style="font-weight:700;">Introduction</h2>

        <p>
            Modern transformers rely heavily on softmax attention, which costs 
            \(O(n^2)\) time and memory. Efficient transformer variants attempt to reduce 
            this complexity. This blog explains how <strong>linear attention</strong> and the 
            <strong>Delta Rule</strong> allow us to compute attention in \(O(n)\) time.
        </p>

        <p>
            We start from the classical attention formulation:
        </p>

        <p style="font-size:1.1rem; text-align:center;">
            \[
            \text{Att}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
            \]
        </p>

        <p>
            Linear attention replaces softmax with a kernel feature map 
            \(\phi(\cdot)\), giving:
        </p>

        <p style="font-size:1.1rem; text-align:center;">
            \[
            \text{LinearAtt}(Q,K,V) = \phi(Q)\left(\phi(K)^T V\right)
            \]
        </p>

        <p>
            This allows us to accumulate information incrementally ‚Äî which is where the 
            <strong>Delta Rule</strong> (Hebbian-style update) comes in:
        </p>

        <p style="font-size:1.1rem; text-align:center;">
            \[
            W_t = W_{t-1} + \phi(k_t)v_t^T
            \]
        </p>

        <hr>


        <!-- Implementing Linear Attention -->
        <h2 id="implement" style="font-weight:700;">Implementing Linear Attention</h2>

        <p>
            Let‚Äôs walk through a minimal PyTorch implementation.  
            Below is a clean, readable version. Click to expand the code.
        </p>

            <!-- Collapsible Code Block 1 -->
            <button class="btn btn-dark btn-sm mb-2" type="button" 
                    data-toggle="collapse" data-target="#code1"
                    aria-expanded="false" aria-controls="code1">
                ‚ñ∂ Show Code
            </button>

            <div class="collapse" id="code1">
                <div class="card card-body" style="background:#f8f8f8; border:1px solid #ccc;">
            
                    <!-- Copy Button -->
                    <button class="btn btn-outline-secondary btn-sm mb-2" 
                            onclick="copyCode('codeblock1')">
                        Copy Code
                    </button>

                    <pre style="white-space:pre-wrap; margin:0; padding:0; text-align:left">
<code id="codeblock1">
import torch
import torch.nn as nn
class LinearAttention(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
    def phi(self, x):
        return torch.nn.functional.elu(x) + 1 # Feature map for linear attention (ELU + 1 example)
    def forward(self, Q, K, V):
        Q, K, V = map(self.phi, (Q, K, V))
        S = torch.zeros(Q.size(-1), V.size(-1)) # Compute KV^T incrementally
        for t in range(K.size(0)):
            S += torch.outer(K[t], V[t])  # Hebbian update
            out = torch.matmul(Q, S)
        return out
</code>
                    </pre>
                </div>
            </div>
        <br>


        <!-- Delta Rule Section -->
        <h2 id="delta" style="font-weight:700;">Why Delta Rule Helps</h2>

        <p>
            The Delta Rule enables incremental updates without storing the entire 
            sequence. Instead of doing:
        </p>

        <p style="font-size:1.1rem; text-align:center;">
            \[
            Q(K^T V)
            \]
        </p>

        <p>
            ‚Ä¶we maintain a running representation:
        </p>

        <p style="font-size:1.1rem; text-align:center;">
            \[
            S_t = S_{t-1} + \phi(k_t)v_t^T
            \]
        </p>

        <p>
            This means attention can be computed in a single sequence pass ‚Äî 
            perfect for long documents and video.
        </p>

        <!-- Collapsible Code Block 2 -->
        <button class="btn btn-dark btn-sm mb-2" type="button" 
                data-toggle="collapse" data-target="#code2"
                aria-expanded="false" aria-controls="code2">
            ‚ñ∂ Show Python Math Example
        </button>

        <div class="collapse" id="code2">
            <div class="card card-body" style="background:#f8f8f8; border:1px solid #ccc;">

                <!-- Copy Button -->
                <button class="btn btn-outline-secondary btn-sm mb-2" 
                        onclick="copyCode('codeblock2')">
                    Copy Code
                </button>

<pre style="white-space:pre-wrap;"><code id="codeblock2">
import torch
phi = lambda x: torch.nn.functional.elu(x) + 1

K = torch.randn(5, 4)  # 5 tokens, dim=4
V = torch.randn(5, 4)
S = torch.zeros(4, 4)

for t in range(5):
    S += torch.outer(phi(K[t]), V[t])
    print("Final S matrix:\n", S)
</code></pre>
            </div>
        </div>

        <hr>


        <!-- References Section -->
        <h2 id="refs" style="font-weight:700;">References</h2>

        <p>
            Linear attention combined with the Delta Rule gives a powerful way to scale 
            transformers to extremely long sequences. It is memory-efficient, fast, and 
            mathematically elegant. Future posts in this series will show:
        </p>

        <ul>
            <li><a href="./blog-part2.html">How the algorithm is parallelized</a></li>
            <li><a href="./blog-part3.html">How the architecture extends to deep stacks</a></li>
        </ul>

    </div>

    <div id="footer" class="mb-5">
        <hr>
        <div class="container text-center">
                <a href="mailto:dawoodsarfraz.cs@gmail.com" class="fas fa-envelope fa-1x" title="E-mail"></a>
                <a href="https://www.linkedin.com/in/dawood-sarfraz-0466541b6/" class="fab fa-linkedin fa-1x" title="LinkedIn" target="_blank"></a>
                <a href="https://github.com/Dawoodsarfraz" class="fab fa-github fa-1x" title="GitHub"></a>
                <a href="https://scholar.google.com" class="ai ai-google-scholar fa-1x" title="Google Scholar"></a>
                <a href="https://www.researchgate.net" class="ai ai-researchgate fa-1x" title="ResearchGate"></a>
                <a href="https://medium.com/@dawoodsarfraz0346" class="fab fa-medium fa-1x" title="Medium" target="_blank"></a>
                <a href="../../../assests/cv/Dawood_Sarfraz_ML_CV.pdf" class="fas fa-file-alt fa-1x" title="CV" target="_blank"></a>
                <br>
                <a id="copyright" class="mb-5">¬© Copyright 2025 Dawood Sarfraz. Hosted by GitHub Pages.</a>
        </div>
    </div>
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        function copyCode(id) {
            let code = document.getElementById(id).innerText;
            navigator.clipboard.writeText(code);
            alert("Code copied!");
        }
    </script>



</body>
</html>
